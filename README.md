# Awesome List Generator üìú‚ú®

This is a Python program that automatically generates an "awesome list" for a specific
keyword as a markdown file. An "awesome list" is a list of resources related to a
specific topic. Currently, the resources include GitHub projects, Google Scholar
articles, YouTube videos, and podcasts. The awesome list is automatically generated
using GPT models; you can choose between different models to generate the list, such as
GPT 3.5 or GPT 4.

## Demo üé•

https://github.com/alialsaeedi19/GPT-Awesome-List-Maker/assets/21360696/e8858303-9108-4235-9fd1-9408e885f8f0

## Setup ‚öôÔ∏è

1. Make sure you are using Python 3.10.
2. Install poetry from the [official site](https://python-poetry.org/docs/#installation).
3. Install dependencies using poetry:
    ```bash
    poetry install
    ```
4. Create an .env file at the root of the project and add the following environment variable:
    ```
    OPENAI_API_KEY=<your_openai_api_key>
    ```

## Usage üíª

### Using Streamlit UI

We've provided a Streamlit interface for running this application. To use it:

1. Run the Streamlit application using Poetry:
    ```bash
    poetry run streamlit run streamlit_run.py
    ```

2. Open `http://localhost:8501`

You can easily input the necessary parameters (like model type, keyword, and description) through the UI and generate
your awesome list!

### Direct Code Usage

The main class used in this project is the `AwesomeListGenerator`. This class accepts the following parameters:

- `keyword`: A string representing the keyword for which the awesome list will be generated.
- `description`: A string providing a description related to the keyword.
- `model`: A string representing the OpenAI model to be used for generating the markdown (default is "
  gpt-3.5-turbo-16k").
- `data_extraction_batch_size`: An integer representing the number of data items to process in each batch (default is
  10). For example, if the batch size is 10, then the data will be fetched from the data sources in batches of 10 (like
  10 github projects at a time).
- `number_of_results`: An integer representing the number of results to fetch from each data source (default is 20). the
  number of results to fetch from each data source (default is 20). For example, fetch 20 Github projects then process
  them with LLM model in batches based on data_extraction_batch_size.

After initializing the class with these parameters, invoke the `save_and_return_awesome_list` method to generate the
markdown file. Here's an example:

```python
# Initialize an instance of the AwesomeListGenerator
generator = AwesomeListGenerator(keyword="Your Keyword",
                                 description="Your Description",
                                 model="gpt-3.5-turbo-16k",
                                 data_extraction_batch_size=10,
                                 number_of_results=20)
# Generate and save the markdown
markdown_content = generator.save_and_return_awesome_list()
```

The program will generate a markdown file in the `output` directory named after your keyword (e.g., `Your_Keyword.md`).
This file contains the "awesome list" generated by the program.

## How It Works üïπÔ∏è

The `AwesomeListGenerator` program operates in two main phases: Data Scraping and Data Processing.

### Data Scraping üï∏Ô∏è

In the data scraping phase, the program fetches resources related to your provided keyword from multiple data sources.
Currently, the resources include GitHub repositories, Google Scholar articles, YouTube videos, and podcasts. The program
utilizes specialized scrapers for each source, each of which is designed to fetch the most relevant and highest quality
resources.

For instance, the GitHub scraper fetches repositories that match the keyword, sorted by the number of stars (a common
indicator of a repository's relevance and quality). Similarly, the Google Scholar scraper retrieves articles related to
the keyword and sorted by citation count.

### Data Processing üß†

Once the data is scraped, it is passed on to the data processing phase. In this phase, the program uses the selected GPT
model to process the fetched resources. The model filters and ranks the resources based on relevance to the keyword,
quality of content, and potential usefulness to users. The GPT model also formats the data into a markdown list, adding
necessary formatting such as links and brief descriptions.

Notably, both scraping and processing operations are executed in batches. This batch-wise operation allows the program
to support as many results as needed, based on the configured `number_of_results` and `data_extraction_batch_size`. This
way, you have control over the extent of data being handled at a time, ensuring efficient resource usage.

## Expansion and Contributions üí°

We're looking to expand the number of data sources in the future. Here are some ideas we have in mind:

- Scrape resources from Medium.
- Search for related books using Google Books or Amazon API.
- Fetch blog posts from dev.to and other developer-focused platforms.
- Retrieve documents from preprint servers like arXiv and bioRxiv.
- Extract relevant resources from online course platforms such as Coursera, Udemy, and Khan Academy.

If you're interested in contributing, you can pick one of the above tasks or propose your own ideas. We welcome all
kinds of contributions and appreciate your interest in our project!

## TODO Checklist

- [ ] Add Medium Scraper
- [ ] Integrate Google Books or Amazon API for books
- [ ] Develop a scraper for dev.to and other developer-focused platforms
- [ ] Design scraper for preprint servers like arXiv and bioRxiv
- [ ] Integrate online course platforms such as Coursera, Udemy, and Khan Academy

## Support Us ‚ù§Ô∏è

Did you find this project useful? If it has brought value to you, please give us a ‚≠ê on GitHub. This gesture not only
validates our efforts but also helps this project reach more people and continue development.

Feel free to fork the repository, contribute by submitting pull requests, or open an issue. Your feedback and
contributions are always welcome!
